shiny::runApp()
set.seed(12345)
logistic=makeLearner("classif.logreg", predict.type="response")
model=train(logistic, trainTask)
pred=predict(model, testTask)
performance(pred, measures=acc)
# Logistic regression
logistic=makeLearner("classif.logreg", predict.type="prob")
# Training the model
model=train(logistic, trainTask)
data=readRDS("/Users/Maxime/Documents/Cours/Master/M2/M2S1/SVM/projetSVM/new.rds")
data$class=as.factor(data$class)
set.seed(12345)
table(data$class)
#taille_ech=175000
taille_ech=1000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.55))
train=data[trainindex,]
validateindex=sample(index,round(taille_ech*0.27))
validate=data[validateindex,]
itest=sample(index,round(taille_ech*0.18))
test=data[itest,]
attach(train)
# Create a task
trainTask=makeClassifTask(data=train, target="class")
testTask=makeClassifTask(data=test, target="class")
validateTask=makeClassifTask(data=validate, target="class")
# Let's consider the positive class as 1
trainTask=makeClassifTask(data=train,target="class", positive="1")
testTask=makeClassifTask(data=test,target="class", positive="1")
validateTask=makeClassifTask(data=validate,target="class", positive="1")
# Let's normalize the variables
trainTask=normalizeFeatures(trainTask,method="standardize")
testTask=normalizeFeatures(testTask,method="standardize")
validateTask=normalizeFeatures(validateTask,method="standardize")
trainTask=makeClassifTask(data=train, target="class")
testTask=makeClassifTask(data=test, target="class")
validateTask=makeClassifTask(data=validate, target="class")
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
library(MASS)
library(ROCR)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
library(ineq)
#paralléliser
library(parallelMap)
#démarrer la parallélisation
parallelStartSocket(cpus=4)
set.seed(12345)
taille_ech=1000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.55))
train=data[trainindex,]
validateindex=sample(index,round(taille_ech*0.27))
validate=data[validateindex,]
itest=sample(index,round(taille_ech*0.18))
test=data[itest,]
attach(train)
trainTask=makeClassifTask(data=train, target="class")
testTask=makeClassifTask(data=test, target="class")
validateTask=makeClassifTask(data=validate, target="class")
trainTask=makeClassifTask(data=train,target="class", positive="1")
testTask=makeClassifTask(data=test,target="class", positive="1")
validateTask=makeClassifTask(data=validate,target="class", positive="1")
trainTask=normalizeFeatures(trainTask,method="standardize")
testTask=normalizeFeatures(testTask,method="standardize")
validateTask=normalizeFeatures(validateTask,method="standardize")
# Logistic regression
logistic=makeLearner("classif.logreg", predict.type="prob")
# Training the model
model=train(logistic, trainTask)
# Predicting on the test set
pred1.test=predict(model, testTask)
# Measuring the performance
performance(pred1.test, measures=acc)
# 0.980254
# Create submission file
submit2=data.frame(class=test$class, class_Status=pred1.test$data$response)
table(submit2$class,submit2$class_Status)
mean(submit2$class==submit2$class_Status)
set.seed(12345)
logistic=makeLearner("classif.logreg", predict.type="response")
model=train(logistic, trainTask)
pred=predict(model, testTask)
performance(pred, measures=acc)
gini=Gini(pred$data$prob.1)
gini
pred$data$truth
gini=Gini(pred$data$id)
gini
gini=Gini(pred$data$truth)
gini
gini=Gini(pred$data$response)
gini
pred1.test$data$prob.1
set.seed(12345)
logistic=makeLearner("classif.logreg", predict.type="response")
model=train(logistic, trainTask)
pred=predict(model, testTask)
performance(pred, measures=acc)
gini=Gini(pred$data$prob.1)
submit2=data.frame(class=test$class, class_Status=pred$data$response)
tab=table(submit2$class,submit2$class_Status)
clas= mean(submit2$class==submit2$class_Status)
gini
gini=Gini(pred$data$response)
gini
set.seed(12345)
minsplit=5
minbucket=15
cp=0.001
getParamSet("classif.rpart")
tree=makeLearner("classif.rpart", predict.type="response")
set_cv=makeResampleDesc("CV", iters=3)
dtparam=makeParamSet(
makeIntegerParam("minsplit", lower=5, upper=50),
makeIntegerParam("minbucket", lower=5, upper=50),
makeNumericParam("cp", lower=0.001, upper=0.5))
gridsearchcontrol=makeTuneControlGrid()
tun.tree=setHyperPars(tree, par.vals=list(minsplit=minsplit,minbucket=minbucket,cp=cp))
tun.rpart=train(tun.tree, trainTask)
treetestpred=predict(tun.rpart, testTask)
treetestpred$data$id
treetestpred$data$truth
treetestpred$data$response
set.seed(12345)
minsplit=5
minbucket=15
cp=0.001
getParamSet("classif.rpart")
tree=makeLearner("classif.rpart", predict.type="response")
set_cv=makeResampleDesc("CV", iters=3)
dtparam=makeParamSet(
makeIntegerParam("minsplit", lower=5, upper=50),
makeIntegerParam("minbucket", lower=5, upper=50),
makeNumericParam("cp", lower=0.001, upper=0.5))
gridsearchcontrol=makeTuneControlGrid()
tun.tree=setHyperPars(tree, par.vals=list(minsplit=minsplit,minbucket=minbucket,cp=cp))
tun.rpart=train(tun.tree, trainTask)
treetestpred=predict(tun.rpart, testTask)
gini=Gini(treetestpred$data$prob.1)
submit3=data.frame(class=test$class, class_Status=treetestpred$data$response)
tab=table(submit3$class,submit3$class_Status)
clas=mean(submit3$class==submit3$class_Status)
treetestpred$data$response
gini=Gini(treetestpred$predict.type)
gini
set.seed(12345)
logistic=makeLearner("classif.logreg", predict.type="response")
model=train(logistic, trainTask)
pred=predict(model, testTask)
performance(pred, measures=acc)
gini=Gini(pred$data$prob.1)
submit2=data.frame(class=test$class, class_Status=pred$data$response)
tab=table(submit2$class,submit2$class_Status)
clas= mean(submit2$class==submit2$class_Status)
set.seed(12345)
ntree=157
nodesize=12
mtry=9
getParamSet("classif.randomForest")
rf=makeLearner("classif.randomForest", predict.type="response", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
rancontrol=makeTuneControlRandom(maxit=10)
set_cv=makeResampleDesc("CV", iters=3)
rf.tree=setHyperPars(rf, par.vals=list(ntree=ntree,mtry=mtry,nodesize=nodesize))
rforest=train(rf.tree, trainTask)
rfmodel=predict(rforest, testTask)
gini=Gini(rfmodel$data$prob.1)
submit4=data.frame(class = test$class, class_Status=rfmodel$data$response)
tab=table(submit4$class,submit4$class_Status)
clas= mean(submit4$class==submit4$class_Status)
gini=(rfmodel$data$response)
gini
gini=Gini(rfmodel$data$response)
gini
gini*100
gini*10
gini=Gini(rfmodel$data$truth)
gini*10
rfmodel$data$truth
rfmodel$data$response
gini=Gini(rfmodel$data$reponse)
gini*10
gini=Gini(rfmodel$data$reponse)
gini*10
gini=Gini(rfmodel$data$truth)
gini*10
gini=Gini(rfmodel$data$response)
gini*10
set.seed(12345)
ntree=157
nodesize=12
mtry=9
getParamSet("classif.randomForest")
rf=makeLearner("classif.randomForest", predict.type="response", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
rancontrol=makeTuneControlRandom(maxit=10)
set_cv=makeResampleDesc("CV", iters=3)
rf.tree=setHyperPars(rf, par.vals=list(ntree=ntree,mtry=mtry,nodesize=nodesize))
rforest=train(rf.tree, trainTask)
rfmodel=predict(rforest, testTask,type="response")
gini=Gini(rfmodel$data$prob.1)
submit4=data.frame(class = test$class, class_Status=rfmodel$data$response)
tab=table(submit4$class,submit4$class_Status)
clas= mean(submit4$class==submit4$class_Status)
gini=Gini(rfmodel$response)
rfmodel$response
rfmodel=predict(rforest, testTask,type="response")
getParamSet("classif.svm")
learner=makeLearner("classif.svm", predict.type="prob")
cv.svm=makeResampleDesc("CV", iters=3, stratify=TRUE)
ctrl=makeTuneControlRandom(maxit=3)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0,upper=1, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
final_svm=setHyperPars(learner=learner, par.vals=list(type="C-classification", kernel="linear", cost=cost, gamma=gamma, shrinking=TRUE))
svm.model=train(final_svm, trainTask)
predict.svm=predict(svm.model, testTask)
Gini=Gini(predict.svm$data$prob.1)
getParamSet("classif.svm")
learner=makeLearner("classif.svm", predict.type="prob")
cv.svm=makeResampleDesc("CV", iters=3, stratify=TRUE)
ctrl=makeTuneControlRandom(maxit=3)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0,upper=1, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
final_svm=setHyperPars(learner=learner, par.vals=list(type="C-classification", kernel="linear", cost=25.8, gamma=0.573, shrinking=TRUE))
svm.model=train(final_svm, trainTask)
predict.svm=predict(svm.model, testTask)
Gini=Gini(predict.svm$data$prob.1)
gini=Gini(rfmodel$dump)
gini*10
set.seed(12345)
ntree=157
nodesize=12
mtry=9
getParamSet("classif.randomForest")
rf=makeLearner("classif.randomForest", predict.type="response", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
rancontrol=makeTuneControlRandom(maxit=10)
set_cv=makeResampleDesc("CV", iters=3)
rf.tree=setHyperPars(rf, par.vals=list(ntree=ntree,mtry=mtry,nodesize=nodesize))
rforest=train(rf.tree, trainTask)
rfmodel=predict(rforest, testTask,type="response")
gini=Gini(rfmodel$data$prob.1)
submit4=data.frame(class = test$class, class_Status=rfmodel$data$response)
tab=table(submit4$class,submit4$class_Status)
clas= mean(submit4$class==submit4$class_Status)
gini=Gini(rfmodel$predict.type)
gini*10
gini=Gini(rfmodel$threshold)
gini*10
gini=Gini(rfmodel$data$id)
gini*10
gini
gini=Gini(rfmodel$data$response)
gini
gini*10
set.seed(12345)
ntree=157
nodesize=12
mtry=9
getParamSet("classif.randomForest")
rf=makeLearner("classif.randomForest", predict.type="response", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
rancontrol=makeTuneControlRandom(maxit=10)
set_cv=makeResampleDesc("CV", iters=3)
rf.tree=setHyperPars(rf, par.vals=list(ntree=ntree,mtry=mtry,nodesize=nodesize))
rforest=train(rf.tree, trainTask)
rfmodel=predict(rforest, testTask,type="prob")
gini=Gini(rfmodel$data$prob.1)
submit4=data.frame(class = test$class, class_Status=rfmodel$data$response)
tab=table(submit4$class,submit4$class_Status)
clas= mean(submit4$class==submit4$class_Status)
rfmodel=predict(rforest, testTask)
shiny::runApp()
set.seed(12345)
nround=256
maxdepth=20
lambda=0.56
eta=0.278
subsample=0.56
minchildweight=4
colsamplebytree=0.683
getParamSet("classif.xgboost")
xg_set=makeLearner("classif.xgboost", predict.type = "prob", nrounds=250,eval_metric = "error",objective = "binary:logistic")
xg_ps=makeParamSet(
makeIntegerParam("nrounds",lower=200,upper=500),
makeIntegerParam("max_depth",lower=3,upper=20),
makeNumericParam("lambda",lower=0.55,upper=0.60),
makeNumericParam("eta", lower = 0.001, upper = 0.5),
makeNumericParam("subsample", lower = 0.10, upper = 0.80),
makeNumericParam("min_child_weight",lower=1,upper=5),
makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8))
rancontrol=makeTuneControlRandom(maxit=5)
set_cv=makeResampleDesc("CV",iters=3)
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=nround,max_depth=maxdepth,lambda=lambda,eta=eta,subsample=subsample,min_child_weight=minchildweight,colsample_bytree=colsamplebytree))
xgmodel=train(xg_new, trainTask)
predict.xg=predict(xgmodel, task=testTask)
gini=Gini(predict.xg$data$prob.1)
submit7=data.frame(class = test$class, class_Status = predict.xg$data$response)
tab=table(submit7$class,submit7$class_Status)
clas=mean(submit7$class==submit7$class_Status)
gini=()
gini
gini=(predict.xg$data$prob.1)
gini
gini=(predict.xg$data$prob.1)
gini
gini=Gini(predict.xg$data$prob.1)
gini
gini=Gini(predict.xg$data$response)
gini*10
gini=Gini(predict.xg$data$truth)
gini*10
set.seed(12345)
logistic=makeLearner("classif.logreg", predict.type="response")
model=train(logistic, trainTask)
pred=predict(model, testTask)
performance(pred, measures=acc)
gini=Gini(pred$data$prob.1)
submit2=data.frame(class=test$class, class_Status=pred$data$response)
tab=table(submit2$class,submit2$class_Status)
clas= mean(submit2$class==submit2$class_Status)
gini=Gini(pred$data$response)
gini*10
runApp()
runApp('~/Desktop/app/final_app')
runApp('~/Desktop/app/final_app')
getParamSet("classif.svm")
learner=makeLearner("classif.svm", predict.type="prob")
cv.svm=makeResampleDesc("CV", iters=3, stratify=TRUE)
ctrl=makeTuneControlRandom(maxit=3)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0,upper=1, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
final_svm=setHyperPars(learner=learner, par.vals=list(type="C-classification", kernel="linear", cost=25.8, gamma=0.573, shrinking=TRUE))
svm.model=train(final_svm, trainTask)
predict.svm=predict(svm.model, testTask)
set.seed(12345)
ntree=157
nodesize=12
mtry=9
getParamSet("classif.randomForest")
rf=makeLearner("classif.randomForest", predict.type="response", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
rancontrol=makeTuneControlRandom(maxit=10)
set_cv=makeResampleDesc("CV", iters=3)
rf.tree=setHyperPars(rf, par.vals=list(ntree=ntree,mtry=mtry,nodesize=nodesize))
rforest=train(rf.tree, trainTask)
rfmodel=predict(rforest, testTask)
gini=Gini(rfmodel$data$prob.1)
submit4=data.frame(class = test$class, class_Status=rfmodel$data$response)
tab=table(submit4$class,submit4$class_Status)
clas= mean(submit4$class==submit4$class_Status)
set.seed(12345)
ntree=157
nodesize=12
mtry=9
getParamSet("classif.randomForest")
rf=makeLearner("classif.randomForest", predict.type="proba", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
rancontrol=makeTuneControlRandom(maxit=10)
set_cv=makeResampleDesc("CV", iters=3)
rf.tree=setHyperPars(rf, par.vals=list(ntree=ntree,mtry=mtry,nodesize=nodesize))
rforest=train(rf.tree, trainTask)
rfmodel=predict(rforest, testTask)
gini=Gini(rfmodel$data$prob.1)
submit4=data.frame(class = test$class, class_Status=rfmodel$data$response)
tab=table(submit4$class,submit4$class_Status)
clas= mean(submit4$class==submit4$class_Status)
set.seed(12345)
ntree=157
nodesize=12
mtry=9
getParamSet("classif.randomForest")
rf=makeLearner("classif.randomForest", predict.type="prob", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
rancontrol=makeTuneControlRandom(maxit=10)
set_cv=makeResampleDesc("CV", iters=3)
rf.tree=setHyperPars(rf, par.vals=list(ntree=ntree,mtry=mtry,nodesize=nodesize))
rforest=train(rf.tree, trainTask)
rfmodel=predict(rforest, testTask)
gini=Gini(rfmodel$data$prob.1)
submit4=data.frame(class = test$class, class_Status=rfmodel$data$response)
tab=table(submit4$class,submit4$class_Status)
clas= mean(submit4$class==submit4$class_Status)
set.seed(12345)
n.trees=256
interaction=5
minobsinnode=33
schrinkage=0.24
getParamSet("classif.gbm")
g.gbm=makeLearner("classif.gbm", predict.type="response")
rancontrol=makeTuneControlRandom(maxit=5)
set_cv=makeResampleDesc("CV",iters=3)
gbm_par=makeParamSet(
makeDiscreteParam("distribution", values="bernoulli"),
makeIntegerParam("n.trees", lower=100, upper=500),
makeIntegerParam("interaction.depth", lower = 2, upper=10),
makeIntegerParam("n.minobsinnode", lower=10, upper=80),
makeNumericParam("shrinkage",lower=0.01, upper=1))
final_gbm=setHyperPars(learner=g.gbm,
par.vals=list(distribution="bernoulli", n.trees=n.trees,
interaction.depth=interaction, n.minobsinnode=minobsinnode,
shrinkage=schrinkage))
to.gbm=train(final_gbm, trainTask)
pr.gbm=predict(to.gbm, testTask)
gini=Gini(pr.gbm$data$prob.1)
submit6=data.frame(class = test$class, class_Status = pr.gbm$data$response)
tab=table(submit6$class,submit6$class_Status)
clas= mean(submit6$class==submit6$class_Status)
gini=Gini(pr.gbm$data$response)
gini
gini*10
runApp('~/Desktop/app/final_app')
runApp('~/Desktop/app/final_app')
set.seed(12345)
logistic=makeLearner("classif.logreg", predict.type="prob")
model=train(logistic, trainTask)
pred=predict(model, testTask)
performance(pred, measures=acc)
gini=Gini(pred$data$prob.1)
submit2=data.frame(class=test$class, class_Status=pred$data$response)
tab=table(submit2$class,submit2$class_Status)
clas= mean(submit2$class==submit2$class_Status)
set.seed(12345)
logistic=makeLearner("classif.logreg", predict.type="prob")
model=train(logistic, trainTask)
pred=predict(model, testTask)
performance(pred, measures=acc)
gini=Gini(pred$data$prob.1)
submit2=data.frame(class=test$class, class_Status=pred$data$response)
tab=table(submit2$class,submit2$class_Status)
clas= mean(submit2$class==submit2$class_Status)
gini
runApp()
runApp()
