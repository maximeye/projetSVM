# # Predicting on a new dataset
predict.xg=predict(xgmodel, validateTask)
# Submission file
submit7=data.frame(class = zozo$Class, class_Status = predict.xg$data$response)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask)
# Submission file
submit7=data.frame(class = zozo$Class, class_Status = predict.xg$data$response)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask)
View(xgmodel)
View(zozoTask)
# Predicting on the test set
predict.xg=predict(xgmodel, testTask)
View(predict.xg)
View(testTask)
# Submission file
submit7=data.frame(class = test$class, class_Status = predict.xg$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask)
View(testTask)
View(zozoTask)
View(xgmodel)
xgmodel[["features"]]
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask)
View(zozoTask)
View(testTask)
View(xgmodel)
View(zozoTask)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask[["env"]][["data"]][[-"Class"]])
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask)
View(zozo)
View(zozoTask[["env"]][["data"]])
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask[["env"]][["data"]])
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask[["task.desc"]][["target"]])
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask[["task.desc"]])
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask[["task"]])
View(zozoTask)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask$data[,imp$Feature])
zozoTask$data
zozoTask$env$data
zozoTask$env$data[,Class]
zozoTask$env$data$Class
# # Predicting on a new dataset
predict.xg=predict(xgmodel, zozoTask$env$data$Class)
is.Task(zozoTask$env$data$Class)
# Predicting on the test set
predict.xg=predict(xgmodel, testTask,task=task)
# Predicting on the test set
predict.xg=predict(xgmodel, task=testTask)
# Submission file
submit7=data.frame(class = test$class, class_Status = predict.xg$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, task=zozoTask$env$data$Class)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, task=zozoTask)
View(testTask)
zozoTask=makeClassifTask(data=zozo, target="Class")
zozoTask=makeClassifTask(data=zozo,target="Class", positive="1")
zozoTask=normalizeFeatures(zozoTask,method="standardize")
# # Predicting on a new dataset
predict.xg=predict(xgmodel, task=zozoTask)
View(zozoTask)
View(testTask)
# list of parameters which can be tuned
getParamSet("classif.svm")
# LEARNERS
learner=makeLearner("classif.svm", predict.type="prob")
# Asset the parmeters to our leaner
final_svm=setHyperPars(learner=learner, par.vals=list(type="C-classification", kernel="radial", cost=25.8, gamma=0.573, shrinking=FALSE))
# TRAINS
svm.model=train(final_svm,trainTask)
# PREDICTIONS
predict.svm=predict(svm.model, testTask)
zozo$Class=as.factor(zozo$Class)
# Submission file
submit5=data.frame(class=test$class, class_status=predict.svm$data$response)
table(submit5$class,submit5$class_status)
mean(submit5$class==submit5$class_status)
# Making some predictions on a new datset
predict.svm=predict(svm.model, zozoTask)
View(zozoTask)
View(zozo)
head(zozo)
View(svm.model)
View(zozoTask)
calculateROCMeasures(predict.svm)
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
library(MASS)
library(ROCR)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
#paralléliser
library(parallelMap)
#démarrer la parallélisation
parallelStartSocket(cpus=4)
set.seed(12345)
######################################################################################################
################################## DEBUT #############################################################
######################################################################################################
zozo=readRDS('C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/newcreditcard.rds')
zozo$Class=as.factor(zozo$Class)
data=readRDS("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/projetSVM/new.rds")
#data=readRDS("/Users/Maxime/Documents/Cours/Master/M2/M2S1/SVM/projetSVM/new.rds")
data$class=as.factor(data$class)
set.seed(12345)
taille_ech=175000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.55))
train=data[trainindex,]
validateindex=sample(index,round(taille_ech*0.27))
validate=data[validateindex,]
itest=sample(index,round(taille_ech*0.18))
test=data[itest,]
attach(train)
trainTask=makeClassifTask(data=train, target="class")
testTask=makeClassifTask(data=test, target="class")
validateTask=makeClassifTask(data=validate, target="class")
zozoTask=makeClassifTask(data=zozo, target="Class")
# Let's consider the positive class as 1
trainTask=makeClassifTask(data=train,target="class", positive="1")
testTask=makeClassifTask(data=test,target="class", positive="1")
validateTask=makeClassifTask(data=validate,target="class", positive="1")
zozoTask=makeClassifTask(data=zozo,target="Class", positive="1")
# Let's normalize the variables
trainTask=normalizeFeatures(trainTask,method="standardize")
testTask=normalizeFeatures(testTask,method="standardize")
validateTask=normalizeFeatures(validateTask,method="standardize")
zozoTask=normalizeFeatures(zozoTask,method="standardize")
# Feature importance of variables
imp_feature=generateFilterValuesData(trainTask, method=c("information.gain","chi.squared"))
plotFilterValues(imp_feature, n.show=20)
plotFilterValues(imp_feature, n.show=20)
# Logistic regression
logistic=makeLearner("classif.logreg", predict.type="prob")
# Training the model
model=train(logistic, trainTask)
# Predicting on the test set
pred1.test=predict(model, testTask)
# Measuring the performance
performance(pred1.test, measures=acc)
# 0.980254
# Create submission file
submit2=data.frame(class=test$class, class_Status=pred1.test$data$response)
table(submit2$class,submit2$class_Status)
# Calculating the false/true positive rates on the test set & ploting the ROC Curve
roclog.test=generateThreshVsPerfData(pred1.test, measures = list(fpr, tpr, acc))
plotROCCurves(roclog.test)
# Making the decision tree
tree=makeLearner("classif.rpart", predict.type="prob")
# Using hyperparameters for modeling
tun.tree=setHyperPars(tree, par.vals=list(minsplit=5,minbucket=15,cp=0.001))
# Train the model
tun.rpart=train(tun.tree, trainTask)
# Make predictions on the test set
treetestpred=predict(tun.rpart, testTask)
# Create a submission file
submit3=data.frame(class=test$class, class_Status=treetestpred$data$response)
table(submit3$class,submit3$class_Status)
mean(submit3$class==submit3$class_Status)
# Calculating the false/true positive rates on the test set & ploting the ROC Curve
roc_dt.test = generateThreshVsPerfData(treetestpred, list(fpr, tpr, acc))
plotROCCurves(roc_dt.test)
# Create a learner
rf=makeLearner("classif.randomForest", predict.type="prob", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
# Building the RF model now & checking its accuracy
# Using hyperparameters for modeling
rf.tree=setHyperPars(rf, par.vals=list(ntree=157,mtry=9,nodesize=12))
# Train a model
rforest=train(rf.tree, trainTask)
# Making some predictions on the test set
rfmodeltest=predict(rforest, testTask)
# Submission file
submit4=data.frame(class = test$class, class_Status=rfmodeltest$data$response)
table(submit4$class,submit4$class_Status)
mean(submit4$class==submit4$class_Status)
# 0.9982222
# Calculating the false/true positive rates on the test set & ploting the ROC Curve
rocrf.test=generateThreshVsPerfData(rfmodeltest, measures = list(fpr, tpr, acc))
plotROCCurves(rocrf.test)
# Creating a learner
learner=makeLearner("classif.svm", predict.type="prob")
# Resampling
cv.svm=makeResampleDesc("CV", iters=3, stratify=TRUE)
# Random search
ctrl=makeTuneControlRandom(maxit=3)
# Asset the parmeters to our leaner
final_svm=setHyperPars(learner=learner, par.vals=list(type="C-classification", kernel="radial", cost=25.8, gamma=0.573, shrinking=FALSE))
# Training a model
svm.model=train(final_svm, trainTask)
# Making some predictions on the test set
predict.svm.test=predict(svm.model, testTask)
# Submission file
submit5=data.frame(class=test$class, class_status=predict.svm.test$data$response)
table(submit5$class,submit5$class_status)
mean(submit5$class==submit5$class_status)
# 0.9996508 de bonne classif
# Calculating the false/true positive rates on the test set & ploting the ROC Curve
rocsvm.test=generateThreshVsPerfData(predict.svm.test, measures = list(fpr, tpr, acc))
plotROCCurves(rocsvm.test)
# Loading GBM
g.gbm=makeLearner("classif.gbm", predict.type="prob")
# Setting parameters
final_gbm=setHyperPars(learner=g.gbm,
par.vals=list(distribution="bernoulli", n.trees=256,
interaction.depth=5, n.minobsinnode=33,
shrinkage=0.244))
# Train
to.gbm=train(final_gbm, trainTask)
# Predicting on the test set
pr.gbm.test=predict(to.gbm, testTask)
# Submission file
submit6=data.frame(class = test$class, class_Status = pr.gbm.test$data$response)
table(submit6$class,submit6$class_Status)
mean(submit6$class==submit6$class_Status)
# 0.9973333
rocgbm.test=generateThreshVsPerfData(pr.gbm.test, measures = list(fpr, tpr, acc))
plotROCCurves(rocgbm.test)
View(rocgbm.test)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob")
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "error",
nrounds = 250)
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# Train model
xgmodel=train(xg_new, trainTask)
# Predicting on the test set
predict.xg.test=predict(xgmodel, task=testTask)
# Submission file
submit7=data.frame(class = test$class, class_Status = predict.xg.test$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
# 0.9991429
rocxgb.test=generateThreshVsPerfData(predict.xg.test, measures = list(fpr, tpr, acc))
plotROCCurves(rocxgb.test)
roc_compare = generateThreshVsPerfData(list(Logistic=pred1.test,Tree=treetestpred,Random_Forest=rfmodeltest,SVM=predict.svm.test,Gradient_Boosting=pr.gbm.test,XGBoost=predict.xg.test), list(fpr, tpr))
plotROCCurves(roc_compare)
#stopper la parallélisation
parallelStop()
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
library(MASS)
library(ROCR)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
#paralléliser
library(parallelMap)
#démarrer la parallélisation
parallelStartSocket(cpus=4)
set.seed(12345)
zozo=readRDS('C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/newcreditcard.rds')
zozo$Class=as.factor(zozo$Class)
data=readRDS("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/projetSVM/new.rds")
#data=readRDS("/Users/Maxime/Documents/Cours/Master/M2/M2S1/SVM/projetSVM/new.rds")
data$class=as.factor(data$class)
set.seed(12345)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob")
getParamSet(classif.xgboost)
View(xg_set)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob")
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "error",
nrounds = 250)
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "auc",
nrounds = 250)
# Defining parameters for tuning
xg_ps=makeParamSet(
makeIntegerParam("nrounds",lower=200,upper=500),
makeIntegerParam("max_depth",lower=3,upper=20),
makeNumericParam("lambda",lower=0.55,upper=0.60),
makeNumericParam("eta", lower = 0.001, upper = 0.5),
makeNumericParam("subsample", lower = 0.10, upper = 0.80),
makeNumericParam("min_child_weight",lower=1,upper=5),
makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8))
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# parameters optimal values
xg_new$x
# Train model
xgmodel=train(xg_new, trainTask)
taille_ech=175000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.55))
train=data[trainindex,]
validateindex=sample(index,round(taille_ech*0.27))
validate=data[validateindex,]
itest=sample(index,round(taille_ech*0.18))
test=data[itest,]
attach(train)
trainTask=makeClassifTask(data=train, target="class")
testTask=makeClassifTask(data=test, target="class")
validateTask=makeClassifTask(data=validate, target="class")
zozoTask=makeClassifTask(data=zozo, target="Class")
# Let's consider the positive class as 1
trainTask=makeClassifTask(data=train,target="class", positive="1")
testTask=makeClassifTask(data=test,target="class", positive="1")
validateTask=makeClassifTask(data=validate,target="class", positive="1")
zozoTask=makeClassifTask(data=zozo,target="Class", positive="1")
# Let's normalize the variables
trainTask=normalizeFeatures(trainTask,method="standardize")
testTask=normalizeFeatures(testTask,method="standardize")
validateTask=normalizeFeatures(validateTask,method="standardize")
zozoTask=normalizeFeatures(zozoTask,method="standardize")
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob")
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "auc",
nrounds = 250)
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# parameters optimal values
xg_new$x
# Train model
xgmodel=train(xg_new, trainTask)
View(xg_set)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob")
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "error",
nrounds = 250)
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# Predicting on the test set
predict.xg.test=predict(xgmodel, task=testTask)
# Train model
xgmodel=train(xg_new, trainTask)
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "error",
nrounds = 20)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob")
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# Train model
xgmodel=train(xg_new, trainTask)
# Predicting on the test set
predict.xg.test=predict(xgmodel, task=testTask)
# Submission file
submit7=data.frame(class = test$class, class_Status = predict.xg.test$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "error",
nrounds = 250)
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# Train model
xgmodel=train(xg_new, trainTask)
# Predicting on the test set
predict.xg.test=predict(xgmodel, task=testTask)
# Submission file
submit7=data.frame(class = test$class, class_Status = predict.xg.test$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob")
View(xg_set)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob", nrounds=250)
View(xg_set)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob", nrounds=250?eval_metric = "error")
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob", nrounds=250,eval_metric = "error")
View(xg_set)
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob", nrounds=250,eval_metric = "error",objective = "binary:logistic")
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# Train model
xgmodel=train(xg_new, trainTask)
# Predicting on the test set
predict.xg.test=predict(xgmodel, task=testTask)
# Submission file
submit7=data.frame(class = test$class, class_Status = predict.xg.test$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
# 0.9991746
rocxgb.test=generateThreshVsPerfData(predict.xg.test, measures = list(fpr, tpr, acc))
plotROCCurves(rocxgb.test)
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
library(MASS)
library(ROCR)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
library(ineq)
#parallÃ©liser
library(parallelMap)
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
library(MASS)
library(ROCR)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
library(ineq)
data=readRDS("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/projetSVM/new.rds")
data$class=as.factor(data$class)
set.seed(12345)
data=read.csv("C:/Users/kevas/Desktop/projetSVM/new.csv",header=T,sep=",")
data=data[,c(-1,-17,-15,-27,-24,-28)] # suppression des var les moins importantes
saveRDS(data,"C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/projetSVM/new.rds",compress=TRUE)
data=readRDS("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/projetSVM/new.rds")
data$class=as.factor(data$class)
set.seed(12345)
shiny::runApp('C:/Users/kevas/Desktop/Cours/M1 bis/Semestre 2/Nouvelles Technologies sous  R/Personnalisation_Training')
runApp('C:/Users/kevas/Desktop/Cours/M1 bis/Semestre 2/Nouvelles Technologies sous  R/Reg2')
# Knit the document, passing in the `params` list, and eval it in a
# child of the global environment (this isolates the code in the document
# from the code in this app).
rmarkdown::render(tempReport, output_file = file,
params = params,
envir = new.env(parent = globalenv())
)
getwd()
<div> <img src="MasterEsa.png" width="200px" align="right"> </div>
img=htmltools::img(src = knitr::image_uri(file.path(R.home("doc"), "html", "logo.jpg")),
alt = 'logo',
style = 'position:absolute; top:50px; right:1%; padding:10px;z-index:200;')
htmlhead=paste0('
<script>
document.write(\'<div class="logos">',img,'</div>\')
</script>
')
readr::write_lines(htmlhead, path="header.html")
img=htmltools::img(src = knitr::image_uri(file.path(R.home("doc"), "html", "logo.jpg")),
alt = 'logo',
style = 'position:absolute; top:50px; right:1%; padding:10px;z-index:200;')
htmlhead=paste0('
<script>
document.write(\'<div class="logos">',img,'</div>\')
</script>
')
readr::write_lines(htmlhead, path="header.html")
img=htmltools::img(src = knitr::image_uri(file.path(R.home("doc"), "html", "logo.jpg")),
alt = 'logo',
style = 'position:absolute; top:50px; right:1%; padding:10px;z-index:200;')
htmlhead=paste0('
<script>
document.write(\'<div class="logos">',img,'</div>\')
</script>
')
readr::write_lines(htmlhead, path="header.html")
img=htmltools::img(src = knitr::image_uri(file.path(R.home("doc"), "html", "logo.jpg")),
alt = 'logo',
style = 'position:absolute; top:50px; right:1%; padding:10px;z-index:200;')
htmlhead=paste0('
<script>
document.write(\'<div class="logos">',img,'</div>\')
</script>
')
readr::write_lines(htmlhead, path="header.html")
setwd("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/projetSVM/final_app")
