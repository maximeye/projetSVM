knitr::opts_chunk$set(echo = TRUE)
set.seed(10111)
library(e1071)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 50, scale = FALSE)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
ygrid = predict(svmfit, xgrid)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
plot(xgrid, col = c("#A41743", "#127063")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y+3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
set.seed(10111)
library(e1071)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 50, scale = FALSE)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
ygrid = predict(svmfit, xgrid)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
plot(xgrid, col = c("#A41743", "#127063")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y+3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
Sys.which('pdflatex')
install.packages('tinytex')
tinytex::install_tinytex()
tinytex::install_tinytex()
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
library(MASS)
library(ROCR)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
library(ineq)
#parallÃ©liser
library(parallelMap)
#dÃ©marrer la parallÃ©lisation
parallelStartSocket(cpus=4)
set.seed(12345)
data=readRDS("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/projetSVM/new.rds")
data$class=as.factor(data$class)
set.seed(12345)
table(data$class)
#taille_ech=175000
taille_ech=1000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.55))
train=data[trainindex,]
validateindex=sample(index,round(taille_ech*0.27))
validate=data[validateindex,]
itest=sample(index,round(taille_ech*0.18))
test=data[itest,]
attach(train)
# Create a task
trainTask=makeClassifTask(data=train, target="class")
testTask=makeClassifTask(data=test, target="class")
validateTask=makeClassifTask(data=validate, target="class")
# Let's consider the positive class as 1
trainTask=makeClassifTask(data=train,target="class", positive="1")
testTask=makeClassifTask(data=test,target="class", positive="1")
validateTask=makeClassifTask(data=validate,target="class", positive="1")
# Let's normalize the variables
trainTask=normalizeFeatures(trainTask,method="standardize")
testTask=normalizeFeatures(testTask,method="standardize")
validateTask=normalizeFeatures(validateTask,method="standardize")
# Feature importance of variables
imp_feature=generateFilterValuesData(trainTask, method=c("information.gain","chi.squared"))
plotFilterValues(imp_feature, n.show=20)
# Creating a learner
learner=makeLearner("classif.svm", predict.type="prob")
# Resampling
cv.svm=makeResampleDesc("CV", iters=3, stratify=TRUE)
# Random search
ctrl=makeTuneControlRandom(maxit=3)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0,upper=1, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
# ArrÃªter la parallÃ©lisation.
parallelStop()
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.01,upper=1, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", values=0.1,requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", default =0.1, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.01, upper=0.01, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.1, upper=0.1, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.01, upper=0.1, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.001, upper=0.5, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.001, upper=0.45, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.001, upper=0.40, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
# Parameters optimal values
svm.res$x
# Resampling
cv.svm=makeResampleDesc("CV", iters=10, stratify=TRUE)
# Random search
ctrl=makeTuneControlRandom(maxit=5)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.001, upper=0.40, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
# Parameters optimal values
svm.res$x
# Resampling
cv.svm=makeResampleDesc("CV", iters=30, stratify=TRUE)
# Random search
ctrl=makeTuneControlRandom(maxit=5)
param.svm=makeParamSet(
makeDiscreteLearnerParam(id="type",values=c("C-classification", "nu-classification")),
makeDiscreteLearnerParam(id="kernel", values=c("linear", "polynomial", "radial", "sigmoid")),
makeNumericLearnerParam(id="cost", lower=1,upper=100, requires=quote(type == "C-classification")),
makeNumericLearnerParam(id="nu", lower=0.001, upper=0.40, requires=quote(type == "nu-classification")),
makeIntegerLearnerParam(id="degree", lower=1,upper=3 ,requires=quote(kernel == "polynomial")),
makeNumericLearnerParam(id="gamma", lower=2^-3,upper=1, requires=quote(kernel != "linear")),
makeLogicalLearnerParam(id="shrinking"))
# Searching the optimal parameters
svm.res=tuneParams(learner, validateTask, resampling=cv.svm,
par.set=param.svm, control=ctrl,measures=acc)
# Parameters optimal values
svm.res$x
# Loading GBM
g.gbm=makeLearner("classif.gbm", predict.type="prob")
# Specify the tuning method
rancontrol=makeTuneControlRandom(maxit=30)
# 3 fold CV
set_cv=makeResampleDesc("CV",iters=5)
# Set tunable parameters
gbm_par=makeParamSet(
makeDiscreteParam("distribution", values="bernoulli"),
makeIntegerParam("n.trees", lower=100, upper=500),
makeIntegerParam("interaction.depth", lower = 2, upper=10),
makeIntegerParam("n.minobsinnode", lower=10, upper=80),
makeNumericParam("shrinkage",lower=0.01, upper=1))
### Tuning parameters
tune_gbm=tuneParams(learner = g.gbm, task = validateTask,resampling = set_cv,
measures = acc,par.set = gbm_par,control = rancontrol)
# Set tunable parameters
gbm_par=makeParamSet(
makeDiscreteParam("distribution", values="bernoulli"),
makeIntegerParam("n.trees", lower=100, upper=500),
makeIntegerParam("interaction.depth", lower = 2, upper=10),
makeIntegerParam("n.minobsinnode", lower=10, upper=50),
makeNumericParam("shrinkage",lower=0.01, upper=1))
### Tuning parameters
tune_gbm=tuneParams(learner = g.gbm, task = validateTask,resampling = set_cv,
measures = acc,par.set = gbm_par,control = rancontrol)
# Checking CV accuracy
tune_gbm$y
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "prob",
nrounds=250, eval_metric = "error", objective = "binary:logistic")
# Defining parameters for tuning
xg_ps=makeParamSet(
makeIntegerParam("nrounds",lower=200,upper=500),
makeIntegerParam("max_depth",lower=3,upper=20),
makeNumericParam("lambda",lower=0.55,upper=0.60),
makeNumericParam("eta", lower = 0.001, upper = 0.5),
makeNumericParam("subsample", lower = 0.10, upper = 0.80),
makeNumericParam("min_child_weight",lower=1,upper=5),
makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8))
# Defining search function
rancontrol=makeTuneControlRandom(maxit=30)
# 3 fold cross validation
set_cv=makeResampleDesc("CV",iters=5)
# Tuning parameters
xg_tune=tuneParams(learner=xg_set, task=validateTask, resampling=set_cv,measures=acc,par.set=xg_ps, control=rancontrol)
# Create a learner
rf=makeLearner("classif.randomForest", predict.type="prob", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
# Set tunable parameters
# Grid search to find hyperparameters
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
# Let's do random search for 10 iterations
rancontrol=makeTuneControlRandom(maxit=30)
# Set 3 fold cross validation
set_cv=makeResampleDesc("CV", iters=5)
# Hypertuning
rf_tune=tuneParams(learner=rf, resampling=set_cv, task=validateTask, par.set=rf_param, control=rancontrol, measures=acc)
# Making the decision tree
tree=makeLearner("classif.rpart", predict.type="prob")
# Set 3 fold cross validation
set_cv=makeResampleDesc("CV", iters=5)
# Searching for some hyperparameters
dtparam=makeParamSet(
makeIntegerParam("minsplit", lower=5, upper=50),
makeIntegerParam("minbucket", lower=5, upper=50),
makeNumericParam("cp", lower=0.001, upper=0.5))
# Grid search
gridsearchcontrol=makeTuneControlGrid()
# Hypertuning the parameters
stune=tuneParams(learner=tree, resampling=set_cv, task=validateTask, par.set=dtparam, control=gridsearchcontrol, measures=acc)
# Grid search
gridsearchcontrol=makeTuneControlGrid()
tree$par.vals=list(importance=TRUE)
rf$par.vals=list(importance=TRUE)
# Create a learner
rf=makeLearner("classif.randomForest", predict.type="prob", par.vals=list(ntree=200, mtry=3))
rf$par.vals=list(importance=TRUE)
# Set tunable parameters
# Grid search to find hyperparameters
rf_param=makeParamSet(
makeIntegerParam("ntree",lower=50,upper=200),
makeIntegerParam("mtry",lower=5,upper=20),
makeIntegerParam("nodesize", lower=10, upper=26))
rf$par.vals
rf_param
rf$par.set
htmltools::img(src = knitr::image_uri(file.path(R.home("doc"), "html", "MasterEsa.png")),
alt = 'logo',
style = 'position:absolute; top:0; right:0; padding:10px;')
htmltools::img(src = knitr::image_uri(file.path(R.home("doc"), "html", "MasterEsa.png")),
alt = 'logo',
style = 'position:absolute; top:0; right:0; padding:10px;')
install.packages("rsconnect")
library(rsconnect)
install.packages("shinyapps")
