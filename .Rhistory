seq(from=min(train$V12),to=max(train$V12),length.out=length))
z=((gamma*(w[1,1]*grid[,1] + w[1,2]*grid[,2]+w[1,3]*grid[,3])+model$coef0))^3
plot3d(grid[,1],grid[,2],z)  # this will draw plane.
# adding of points to the graphics.
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
prediction=predict(model, test[,-4])
(tab=table(pred = prediction, true=test[,4]))
model=svm(class~., data=train, kernel="radial",gamma = 0.01, cost = 10)
w <- t(model$coefs) %*% model$SV
####Taux de bonne prediction sur echantillon test
Y=predict(model,newdata = test)
table(test$class,Y)
mean(test$class==Y)
mean(test$class!=Y)
####Taux de bonnes/mauvaises classifications sur echantillon complet:
dataa=data[1:60000,]
Y=predict(model,newdata = dataa)
table(dataa$class,Y)
mean(dataa$class==Y)
mean(dataa$class!=Y)
#Plot echantillon APPRENTISSAGE
colors =c("blue","red")
p3d<- plot3d(train$V12, train$V14, train$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(train$V12, train$V14, train$V17, cex=0.5, adj = 1)
#Plot echantillon de TEST
colors =c("blue","red")
p3d<- plot3d(test$V12, test$V14, test$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(test$V12, test$V14, test$V17, cex=0.5, adj = 1)
length = 100
grid = expand.grid(seq(from=min(train$V17),to=max(train$V17),length.out=length),
seq(from=min(train$V14),to=max(train$V14),length.out=length))
#z = (model$rho- w[1,1]*grid[,1] - w[1,2]*grid[,2]) / w[1,3]
z= exp((-gamma)*(w[1,1]*grid[,1] - w[1,2]*grid[,2])^2)
plot3d(grid[,1],grid[,2],z)  # this will draw plane.
# adding of points to the graphics.
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
model=svm(class~., data=train, kernel="radial",gamma = 0.01, cost = 10)
w <- t(model$coefs) %*% model$SV
####Taux de bonne prediction sur echantillon test
Y=predict(model,newdata = test)
table(test$class,Y)
mean(test$class==Y)
mean(test$class!=Y)
####Taux de bonnes/mauvaises classifications sur echantillon complet:
dataa=data[1:60000,]
Y=predict(model,newdata = dataa)
table(dataa$class,Y)
mean(dataa$class==Y)
mean(dataa$class!=Y)
#Plot echantillon APPRENTISSAGE
colors =c("blue","red")
p3d<- plot3d(train$V12, train$V14, train$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(train$V12, train$V14, train$V17, cex=0.5, adj = 1)
#Plot echantillon de TEST
colors =c("blue","red")
p3d<- plot3d(test$V12, test$V14, test$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(test$V12, test$V14, test$V17, cex=0.5, adj = 1)
length = 100
grid = expand.grid(seq(from=min(train$V17),to=max(train$V17),length.out=length),
seq(from=min(train$V14),to=max(train$V14),length.out=length))
#z = (model$rho- w[1,1]*grid[,1] - w[1,2]*grid[,2]) / w[1,3]
z= exp((-gamma)*(model$rho- w[1,1]*grid[,1] - w[1,2]*grid[,2])^2)
plot3d(grid[,1],grid[,2],z)  # this will draw plane.
# adding of points to the graphics.
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
model=svm(class~., data=train, kernel="polynomial",gamma = 0.01, cost = 10)
w <- t(model$coefs) %*% model$SV
####Taux de bonne prediction sur echantillon test
Y=predict(model,newdata = test)
table(test$class,Y)
mean(test$class==Y)
mean(test$class!=Y)
####Taux de bonnes/mauvaises classifications sur echantillon complet:
dataa=data[1:60000,]
Y=predict(model,newdata = dataa)
table(dataa$class,Y)
mean(dataa$class==Y)
mean(dataa$class!=Y)
#Plot echantillon APPRENTISSAGE
colors =c("blue","red")
p3d<- plot3d(train$V12, train$V14, train$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(train$V12, train$V14, train$V17, cex=0.5, adj = 1)
#Plot echantillon de TEST
colors =c("blue","red")
p3d<- plot3d(test$V12, test$V14, test$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(test$V12, test$V14, test$V17, cex=0.5, adj = 1)
length = 100
grid = expand.grid(seq(from=min(train$V17),to=max(train$V17),length.out=length),
seq(from=min(train$V14),to=max(train$V14),length.out=length))
z=((gamma*(w[1,1]*grid[,1] + w[1,2]*grid[,2])+model$coef0))^2
plot3d(grid[,1],grid[,2],z)  # this will draw plane.
# adding of points to the graphics.
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
prediction=predict(model, test[,-4])
(tab=table(pred = prediction, true=test[,4]))
model=svm(class~., data=train, kernel="sigmoid",gamma = 0.01, cost = 10)
w <- t(model$coefs) %*% model$SV
####Taux de bonne prediction sur echantillon test
Y=predict(model,newdata = test)
table(test$class,Y)
mean(test$class==Y)
mean(test$class!=Y)
####Taux de bonnes/mauvaises classifications sur echantillon complet:
dataa=data[1:60000,]
Y=predict(model,newdata = dataa)
table(dataa$class,Y)
mean(dataa$class==Y)
mean(dataa$class!=Y)
#Plot echantillon APPRENTISSAGE
colors =c("blue","red")
p3d<- plot3d(train$V12, train$V14, train$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(train$V12, train$V14, train$V17, cex=0.5, adj = 1)
#Plot echantillon de TEST
colors =c("blue","red")
p3d<- plot3d(test$V12, test$V14, test$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(test$V12, test$V14, test$V17, cex=0.5, adj = 1)
length = 100
grid = expand.grid(seq(from=min(train$V17),to=max(train$V17),length.out=length),
seq(from=min(train$V14),to=max(train$V14),length.out=length))
z=tan((gamma*(w[1,1]*grid[,1] + w[1,2]*grid[,2]))+model$coef0)
plot3d(grid[,1],grid[,2],z)  # this will draw plane.
# adding of points to the graphics.
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
data=read.csv("/Users/Maxime/Documents/Cours/Master/M2/S1/SVM/Docs Projet/newdat.csv",header=T,sep=",")
data$class=as.factor(data$class)
set.seed(12345)
data=data[,-1]
taille_ech=10000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.7))
train=data[trainindex,]
itest=sample(index,round(taille_ech*0.3))
test=data[itest,]
attach(train)
model=svm(class~.,data=train,kernel="linear",scale=F,cost=105)
w <- t(model$coefs) %*% model$SV
#Taux de bonnes/mauvaises classifications sur echantillon test:
Y=predict(model,newdata = test)
#list(cbind(test, Y), model$index)
table(test$class,Y)
mean(test$class==Y)
mean(test$class!=Y)
#Taux de bonnes/mauvaises classifications sur echantillon complet:
dataa=data[1:60000,]
Y=predict(model,newdata = dataa)
#list(cbind(test, Y), model$index)
table(dataa$class,Y)
mean(dataa$class==Y)
mean(dataa$class!=Y)
#Plot echantillon APPRENTISSAGE
colors =c("blue","red")
p3d<- plot3d(train$V12, train$V14, train$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(train$V12, train$V14, train$V17, cex=0.5, adj = 1)
#Plot echantillon de TEST
colors =c("blue","red")
p3d<- plot3d(test$V12, test$V14, test$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(test$V12, test$V14, test$V17, cex=0.5, adj = 1)
length = 100
grid = expand.grid(seq(from=min(train$V17),to=max(train$V17),length.out=length),
seq(from=min(train$V14),to=max(train$V14),length.out=length))
z = (model$rho- w[1,1]*grid[,1] - w[1,2]*grid[,2]) / w[1,3]
#z=(w[1,1]*grid[,1] + w[1,2]*grid[,2])
plot3d(grid[,1],grid[,2],z)  # this will draw the plane
# adding of points to the graphics.
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
#10 Fold cross validation to determine the optimal gamma and C (cost) in the SVM
#Best parameters : Gamma : 0.01  / Cost (C) : 10
attach(train)
tuned = tune.svm(class~., data = train, gamma = 10^(-6:-1), cost = 10^(-1:1))
summary(tuned)
gamma=0.01
C=10
model=svm(class~., data=train, kernel="radial",gamma = 0.01, cost = 10)
w <- t(model$coefs) %*% model$SV
####Taux de bonne prediction sur echantillon test
Y=predict(model,newdata = test)
table(test$class,Y)
mean(test$class==Y)
mean(test$class!=Y)
####Taux de bonnes/mauvaises classifications sur echantillon complet:
dataa=data[1:60000,]
Y=predict(model,newdata = dataa)
table(dataa$class,Y)
mean(dataa$class==Y)
mean(dataa$class!=Y)
#Plot echantillon APPRENTISSAGE
colors =c("blue","red")
p3d<- plot3d(train$V12, train$V14, train$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(train$V12, train$V14, train$V17, cex=0.5, adj = 1)
#Plot echantillon de TEST
colors =c("blue","red")
p3d<- plot3d(test$V12, test$V14, test$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(test$V12, test$V14, test$V17, cex=0.5, adj = 1)
length = 50
grid = expand.grid(seq(from=min(train$V17),to=max(train$V17),length.out=length),
seq(from=min(train$V14),to=max(train$V14),length.out=length))
z= exp((-gamma)*(model$rho- w[1,1]*grid[,1] - w[1,2]*grid[,2])^2)
plot3d(grid[,1],grid[,2],z)
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
data=read.csv("/Users/Maxime/Documents/Cours/Master/M2/S1/SVM/Docs Projet/newdat.csv",header=T,sep=",")
data$class=as.factor(data$class)
set.seed(12345)
data=data[,-1]
taille_ech=10000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.7))
train=data[trainindex,]
itest=sample(index,round(taille_ech*0.3))
test=data[itest,]
attach(train)
model=svm(class~.,data=train,kernel="linear",scale=F,cost=105)
w <- t(model$coefs) %*% model$SV
#Taux de bonnes/mauvaises classifications sur echantillon test:
Y=predict(model,newdata = test)
#list(cbind(test, Y), model$index)
table(test$class,Y)
mean(test$class==Y)
mean(test$class!=Y)
#Taux de bonnes/mauvaises classifications sur echantillon complet:
dataa=data[1:60000,]
Y=predict(model,newdata = dataa)
#list(cbind(test, Y), model$index)
table(dataa$class,Y)
mean(dataa$class==Y)
mean(dataa$class!=Y)
#Plot echantillon APPRENTISSAGE
colors =c("blue","red")
p3d<- plot3d(train$V12, train$V14, train$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(train$V12, train$V14, train$V17, cex=0.5, adj = 1)
#Plot echantillon de TEST
colors =c("blue","red")
p3d<- plot3d(test$V12, test$V14, test$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(test$V12, test$V14, test$V17, cex=0.5, adj = 1)
length = 100
grid = expand.grid(seq(from=min(train$V17),to=max(train$V17),length.out=length),
seq(from=min(train$V14),to=max(train$V14),length.out=length))
z = (model$rho- w[1,1]*grid[,1] - w[1,2]*grid[,2]) / w[1,3]
#z=(w[1,1]*grid[,1] + w[1,2]*grid[,2])
plot3d(grid[,1],grid[,2],z)  # this will draw the plane
# adding of points to the graphics.
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
#10 Fold cross validation to determine the optimal gamma and C (cost) in the SVM
#Best parameters : Gamma : 0.01  / Cost (C) : 10
attach(train)
gamma=0.01
C=10
model=svm(class~., data=train, kernel="polynomial",gamma = 0.01, cost = 10)
w <- t(model$coefs) %*% model$SV
####Taux de bonne prediction sur echantillon test
Y=predict(model,newdata = test)
table(test$class,Y)
mean(test$class==Y)
mean(test$class!=Y)
####Taux de bonnes/mauvaises classifications sur echantillon complet:
dataa=data[1:60000,]
Y=predict(model,newdata = dataa)
table(dataa$class,Y)
mean(dataa$class==Y)
mean(dataa$class!=Y)
#Plot echantillon APPRENTISSAGE
colors =c("blue","red")
p3d<- plot3d(train$V12, train$V14, train$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(train$V12, train$V14, train$V17, cex=0.5, adj = 1)
#Plot echantillon de TEST
colors =c("blue","red")
p3d<- plot3d(test$V12, test$V14, test$V17, xlab="V12", ylab="V14",
zlab="V17",type="s",radius =0.3,
col=as.integer(train$class) ,
box=FALSE, size=5)
text3d(test$V12, test$V14, test$V17, cex=0.5, adj = 1)
length = 100
grid = expand.grid(seq(from=min(train$V17),to=max(train$V17),length.out=length),
seq(from=min(train$V14),to=max(train$V14),length.out=length))
z=((gamma*(w[1,1]*grid[,1] + w[1,2]*grid[,2])+model$coef0))^2
plot3d(grid[,1],grid[,2],z)  # this will draw plane.
# adding of points to the graphics.
points3d(train$V17[which(train$class==0)], train$V14[which(train$class==0)], train$V12[which(train$class==0)], col='red')
points3d(train$V17[which(train$class==1)], train$V14[which(train$class==1)], train$V12[which(train$class==1)], col='blue')
prediction=predict(model, test[,-4])
(tab=table(pred = prediction, true=test[,4]))
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
#dat=read.csv("/Users/Maxime/Documents/Cours/Master/M2/S1/SVM/Docs Projet/creditcard.csv",header=T,sep=",")
dat=read.csv("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/creditcard.csv",header=T,sep=",")
shiny::runApp('SVM-R-T-Y')
library(shiny)
runApp('SVM-R-T-Y')
runApp('SVM-R-T-Y')
source("Projet R.R")
source("Projet R.R")
glm.fit=glm(Class~.,data=data,family=gaussian,subset=train)
attach(dat)
# On change le type de la variable de reponse "Class" (integer -> factor)
dat$Class=as.factor(dat$Class)
#### Process de selection de variables les plus significatives ####
set.seed(12345)
#dat=read.csv("/Users/Maxime/Documents/Cours/Master/M2/S1/SVM/Docs Projet/creditcard.csv",header=T,sep=",")
dat=read.csv("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/creditcard.csv",header=T,sep=",")
attach(dat)
# On change le type de la variable de reponse "Class" (integer -> factor)
dat$Class=as.factor(dat$Class)
is.factor(dat$Class)
dat
head(dat)
data=read.csv("C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/newdat.csv",header=T,sep=",")
rm(dat)
data$class=as.factor(data$class)
head(data)
install.packages(c("gbm", "xgboost"))
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
library(MASS)
library(ROCR)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
library(data.table)
library(rJava)
set.seed(12345)
data=read.csv("/Users/Maxime/Documents/Cours/Master/M2/S1/SVM/Docs Projet/new.csv",header=T,sep=",")
data=read.csv("/Users/Maxime/Documents/Cours/Master/M2/M2S1/SVM/Docs Projet/new.csv",header=T,sep=",")
data$class=as.factor(data$class)
set.seed(12345)
data=data[,-1]
taille_ech=10000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.7))
train=data[trainindex,]
itest=sample(index,round(taille_ech*0.3))
test=data[itest,]
attach(train)
#create a task
trainTask = makeClassifTask(data = train,target = "class")
testTask = makeClassifTask(data = test, target = "class")
# Let's consider the positive class as 1
trainTask = makeClassifTask(data = train,target = "class", positive = "1")
# Let's normalize the variables
trainTask = normalizeFeatures(trainTask,method = "standardize")
testTask = normalizeFeatures(testTask,method = "standardize")
# Feature importance
im_feat=generateFilterValuesData(trainTask, method = c("information.gain","chi.squared"))
plotFilterValues(im_feat,n.show = 20)
qda=makeLearner("classif.qda", predict.type = "response")
# Train model
qmodel=train(qda, trainTask)
# Predict on test data
qpredict=predict(qmodel, testTask)
# Create submission file
submit1=data.frame(class=test$class,class_status=qpredict$data$response)
write.csv(submit1, "C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/submit1.csv",row.names = F)
table(submit1$class,submit1$class_status)
mean(submit1$class==submit1$class_status)
# Logistic regression
logistic=makeLearner("classif.logreg",predict.type = "response")
# Cross validation (cv) accuracy
cv.logistic=crossval(learner=logistic,task=trainTask,iters = 3,stratify = TRUE,measures = acc,show.info = F)
# Cross validation accuracy
cv.logistic$aggr
# Train model
fmodel=train(logistic,trainTask)
getLearnerModel(fmodel)
# Predict on test data
fpmodel=predict(fmodel, testTask)
# Create submission file
submit2=data.frame(class = test$class, class_Status = fpmodel$data$response)
write.csv(submit2, "C:/Users/kevas/Desktop/Cours/M2/Support_Vector_Machine/Dossier_SVM/submit2.csv",row.names = F)
table(submit2$class,submit2$class_Status)
mean(submit2$class==submit2$class_Status)
getParamSet("classif.rpart")
# Making the decision tree
tree=makeLearner("classif.rpart", predict.type = "response")
# Set 3 fold cross validation
set_cv=makeResampleDesc("CV",iters = 3L)
# Searching for some hyperparameters
gs=makeParamSet(
makeIntegerParam("minsplit",lower = 10, upper = 50),
makeIntegerParam("minbucket", lower = 5, upper = 50),
makeNumericParam("cp", lower = 0.001, upper = 0.2))
# Grid search
gscontrol=makeTuneControlGrid()
# Hypertune the parameters
stune=tuneParams(learner=tree, resampling = set_cv, task = trainTask, par.set = gs, control = gscontrol, measures = acc)
# Cross validation result
stune$y
getParamSet("classif.randomForest")
# Create a learner
rf=makeLearner("classif.randomForest", predict.type = "response", par.vals = list(ntree = 200, mtry = 3))
rf$par.vals=list(importance = TRUE)
# set tunable parameters
# Grid search to find hyperparameters
rf_param=makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 500),
makeIntegerParam("mtry", lower = 3, upper = 10),
makeIntegerParam("nodesize", lower = 10, upper = 50))
# Let's do random search for 50 iterations
rancontrol=makeTuneControlRandom(maxit = 50L)
# Set 3 fold cross validation
set_cv=makeResampleDesc("CV",iters = 3L)
# Hypertuning
rf_tune=tuneParams(learner = rf, resampling = set_cv, task = trainTask, par.set = rf_param, control = rancontrol, measures = acc)
# cv accuracy
rf_tune$y
# The best parameters
rf_tune$x
# Using hyperparameters for modeling
rf.tree=setHyperPars(rf, par.vals = rf_tune$x)
# Train a model
rforest=train(rf.tree, trainTask)
getLearnerModel(t.rpart)
# Making some predictions
rfmodel=predict(rforest, testTask)
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(ROCR)
library(leaps)
library(caTools)
library(MASS)
library(ROCR)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
set.seed(12345)
data=read.csv("/Users/Maxime/Documents/Cours/Master/M2/S1/SVM/Docs Projet/new.csv",header=T,sep=",")
data=read.csv("/Users/Maxime/Documents/Cours/Master/M2/M2S1/SVM/Docs Projet/new.csv",header=T,sep=",")
