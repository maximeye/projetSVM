---
title: "MANUAL"
author: "ROBLIN Caroline, TELLIER Kevin, YE Maxime"
date: '`r format(Sys.time(), "%d %B %Y")`'
output: 
  html_document:
    theme: journal
    highlight: tango

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


###Data presentation

&nbsp;
```{r echo = FALSE,  out.width = "45%", fig.align = 'center', fig.cap = "Data presentation"}
knitr::include_graphics("C:/Users/Caroline/Documents/M2/SVM/projet/projetSVM/onglet1.png")
```
&nbsp;

We first have the dimension of the data table : first the number of observation and after the number of the variables.
We then made a significant test of the variables : first by information gain and after by chi-square test on each of the variables.
&nbsp;

###Comparison of SVM and another model 

&nbsp;
```{r echo = FALSE,  out.width = "45%", fig.align = 'center', fig.cap = "comparison of SVM and another model"}
knitr::include_graphics("C:/Users/Caroline/Documents/M2/SVM/projet/projetSVM/onglet2.png")
```
&nbsp;

####__Inputs__
&nbsp;

#####_Model to compare with svm_
&nbsp;
You have just to select the model with you would compare the SVM among:

* Logistic Regression
* Decision Tree
* Random Forest
* Gradient Boosting
* XGBoost

&nbsp;

When you select another model of logistic regression, you have beside parameters than you can change if you would, but they are selected in a way to they are optimal when you choose your model. The list of parameters (with the default value as a reminder if you change that and want recover them) is :   

* for Decision Tree  
Minsplit : represents the minimum number of observation in a node for a split to take place (5).  
Minbucket : says the minimum number of observation I should keep in terminal nodes (15).  
Cp : is the complexity parameter (0.001).  
* for Random Forest
Number of trees (157).  
Node Size (12).  
Mtry (9).  
* For Gradient Boosting
N trees (256).  
interaction depth (5).  
Min obs in node : refers to the minimum number of observations in a tree node (33).  
shrinkage : is the regulation parameter which dictates how fast / slow the algorithm should move  (0.24).  
* for XGBoost
Nround (256).  
Max depth (20).  
Lambda (0.56).  
Eta (0.278).  
Sub sample (0.56).  
Min child weight (4).  
Cold sample by tree (0.683).  
&nbsp;

#####_SVM Kernel_
&nbsp;
You can select your choice into the type of kernel :

* Linear
* Polynomial
* Radial Basis
* Sigmoid  

The best kernel is selected by default, and is  linear. Optimal parameters are given but you can change that to observe the result.
&nbsp;

#####_Sample size (the larger the size chosen,the longer the processing time will be)_
&nbsp;
For more rapidity is in 1,000 observationsin the begining, but you can vary that with the cursor.
&nbsp;

#####_C_
&nbsp;
As a reminder the optimal C  is 25.8.
&nbsp;

#####_Gamma_
&nbsp;
As a reminder the optimal gamma is 0.573.
&nbsp;

####__Outputs__
&nbsp;

You could observe following

* SVM : Gini coefficient
* SVM : confusion matrix
* SVM : good classification rate
* Selected model : Gini coefficient
* Selected model : Confusion matrix
* Selected model : good classification rate
* ROC Curve comparison between the SVM abd the selected model

You have to be patient when you change the model selected to compare : Warning message can appear, but it's just for the loading time.
&nbsp;

###Comparison of each model's performance - ROC Curve 

&nbsp;
```{r echo = FALSE,  out.width = "45%", fig.align = 'center', fig.cap = "Comparison of each model's performance - ROC Curve"}
knitr::include_graphics("C:/Users/Caroline/Documents/M2/SVM/projet/projetSVM/onglet3.png")
```
&nbsp;

####__Inputs__
&nbsp;

#####_Sample size (the larger the size chosen,the longer the processing time will be)_
&nbsp;
For more rapidity is in 1,000 observations in the begining, but you can vary that with the cursor.
&nbsp;

####__Outputs__
&nbsp;

You can observe in the same graphic the ROC Curve for each models with optimal parameters , and you can remark that it isn't easy to choose the best model regardless of the sample size because of the crossing. It's better to refer to the gini index or the good classification rate we have in the previous tab.  
&nbsp;