---
title: "User manual"
author: "Made by : ROBLIN Caroline, TELLIER Kevin, YE Maxime"
date: '`r format(Sys.time(), "%d %B %Y")`'
output:
  html_document:
    highlight: tango
    theme: journal
  pdf_document: default
---


```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri(file.path("MasterEsa.png")), 
               alt = 'logo', 
               style = 'position:absolute; top:0; right:0; padding:10px;')
```


&nbsp;

&nbsp;


<strong>The goal of this demonstrator is to compare the SVM machine learning method with other machine learning methods.</strong>

&nbsp;

<span style="color:red">Note : the loading of the application depends on the network connection.  
Depending on the network speed, the application may take further time to load.</span>

&nbsp;


### <span style="color: #127063">Explanations of the methodology</span>

&nbsp;

<center>

![1<sup>st</sup> panel : Brief description of SVM and methodology](onglet0.png)

</center>

&nbsp;

<p style="text-align:justify";>In a first part, we briefly explain the concept of SVMs and in a second part the different kernels.  
In the last part, we describe the various modifications made to the initial database and the methodology used to build the demonstrator. </p>


### <span style="color: #127063">Data presentation</span>

&nbsp;

<center>

![2<sup>nd</sup> panel : Data presentation](onglet1.png)

</center>

&nbsp;

<p style="text-align:justify";>First, you have the dimension of the dataset. So we have 316295 observations and all the 31 the variables (the output & 30 explanatory variables).
Then, you can observe two histograms, which represent on the left the importance of the 30 independant variables, given by the information gain ; and on the right side, the result of a significant test is shown, based on the Khi-2 test.</p>


<p style="text-align:justify";>Before continuing, it's important to specify that for models to work properly, the  <span style="color: #127063">SMOTE</span> function was used to resample the original sample which had rare default (when the occurrence 1 is near to 1%).
This function creates other observations with the 1 occurence, using the probability to determine the values of the other variables. So you go from 284315 with the value of class 0 and 492 with the value of class 1, to 284315 with the value of class 0 and 31980 with the value of class 1, because we smote to have 10% of the occurence 1.</p>
&nbsp;

### <span style="color: #127063">Comparison of SVM with another model</span>

&nbsp;

<center>

![3<sup>rd</sup> panel : Comparing SVM with another Machine Learning model](onglet2.png)

</center>

&nbsp;



#### <span style="color: #127063">__Inputs__</span>

&nbsp;


##### <span style="color: #127063">_Model to compare with svm_</span>

&nbsp;

<p style="text-align:justify";>You just have to select the model you want to compare with the SVM, as the : </p>

- Logistic Regression
- Decision Tree
- Random Forest
- Gradient Boosting
- XGBoost

&nbsp;

<p style="text-align:justify";>When you select another model than the logistic regression, beside,  you have parameters than you can change as you wish. They are selected in a way that they are optimal when you choose your model. The list of parameters (with the default value in brackets, as a reminder if you change that and want recover them) is : </p>

* for the Decision Tree : </p>
Minsplit : represents the minimum number of observations in a node for a split to take place (35) </p>
Minbucket : says the minimum number of observations I should keep in terminal nodes (10) </p>
Cp : it's the complexity parameter (0.167) </p>

* for the Random Forest : </p>
Number of trees (108) </p>  
Node Size (11) </p>
Mtry 11 </p>

* for the Gradient Boosting : </p>
N trees (414) </p>
interaction depth (7) </p>
Min obs in node : refers to the minimum number of observations in a tree node (17) </p>  
shrinkage : it's the regulation parameter which dictates how fast / slow the algorithm should move  (0.268). </p>

* for the XGBoost : </p>
Nround (481) </p>
Max depth (16) </p>
Lambda (0.563) </p>
Eta (0.183) </p>
Sub sample (0.328) </p>
Min child weight (1.83) </p>
Cold sample by tree (0.41) </p>

&nbsp;

##### <span style="color: #127063">_SVM Kernel_</span>

&nbsp;

<p style="text-align:justify";>You can choose between those four kernels : </p>

- Linear </p>
- Polynomial </p>
- Radial Basis </p>
- Sigmoid </p>


<p style="text-align:justify";>The best kernel is linear, and it's selected by default. The optimal parameters are given by default aswell, but you can change them as you like and observe the result.</p>

&nbsp;

##### <span style="color: #127063">_Kernel_</span>

<p style="text-align:justify";>As a reminder, the optimal kernel is linear. </p>

&nbsp;

##### <span style="color: #127063">_C_</span>

<p style="text-align:justify";>As a reminder, the optimal C is 42.03045. </p>

&nbsp;

##### <span style="color: #127063">_Sample size (the larger the size chosen, the longer the processing time will be)_</span>

<p style="text-align:justify";>To get the results faster, the default observation value is set 1,000 observations, but you can aswell change it with the cursor.</p>

Note that the sample size chosen has been split into two samples, with 70% of the data for the train dataset, and 30% for the test dataset.
<span style="color:red">The performances displayed are based on the test sample. </span>

&nbsp;

&nbsp;

#### <span style="color: #127063">__Outputs__</span>

&nbsp;

<p style="text-align:justify";>You could observe the following : </p>

- SVM : Gini coefficient </p>
- SVM : confusion matrix </p>
- SVM : good classification rate </p>
- Selected model : Gini coefficient </p>
- Selected model : Confusion matrix </p>
- Selected model : good classification rate </p>
- ROC Curve comparison between the SVM and the selected model </p>


<p style="text-align:justify";>You have to be patient when you change the model selected to be compared to the SVM : <span style="color:red">Warning message can appear</span>, but it's just for the loading time. </p>
&nbsp;

### <span style="color: #127063">Comparison of each model's performance - ROC Curve</span>

&nbsp;

<center>

![4<sup>th</sup> panel : Comparing model performance](onglet3.png)

</center>

&nbsp;

#### <span style="color: #127063">__Inputs__</span>

&nbsp;

##### <span style="color: #127063">_Sample size (the larger the size chosen,the longer the processing time will be)_</span>

<p style="text-align:justify";>To get the results faster, the default observation value is set 1,000 observations, but you can aswell change it with the cursor.</p>

&nbsp;

#### <span style="color: #127063">__Outputs__</span>

&nbsp;

<p style="text-align:justify";>You can observe in the same graphic the ROC curve for each model, with the default optimal parameters.</p>
You can see that it isn't easy to choose the best model regardless of the sample size because of the crossing. It's better to refer to the Gini index or the good classification rate we have in the previous tab.  </p>
&nbsp;
