######################################################################################################
################################## DEBUT #############################################################
######################################################################################################
zozo=read.table('C:/Users/Caroline/Documents/M2/SVM/projet/creditcard.csv',header=T,sep=",")
zozo=zozo[,-c(16,14,26,23,27)]
zozo$Class=as.factor(zozo$Class)
data=readRDS("C:/Users/Caroline/Documents/M2/SVM/projet/new.rds")
data$class=as.factor(data$class)
set.seed(12345)
taille_ech=175000
index=1:nrow(data)
trainindex=sample(index,round(taille_ech*0.55))
train=data[trainindex,]
validateindex=sample(index,round(taille_ech*0.27))
validate=data[validateindex,]
itest=sample(index,round(taille_ech*0.18))
attach(train)
# Create a task
trainTask=makeClassifTask(data=train, target="class")
testTask=makeClassifTask(data=test, target="class")
validateTask=makeClassifTask(data=validate, target="class")
test=data[itest,]
zozoTask=makeClassifTask(data=zozo, target="Class")
# Let's consider the positive class as 1
trainTask=makeClassifTask(data=train,target="class", positive="1")
testTask=makeClassifTask(data=test,target="class", positive="1")
zozoTask=makeClassifTask(data=zozo,target="Class", positive="1")
# Let's normalize the variables
trainTask=normalizeFeatures(trainTask,method="standardize")
testTask=normalizeFeatures(testTask,method="standardize")
validateTask=normalizeFeatures(validateTask,method="standardize")
zozoTask=normalizeFeatures(zozoTask,method="standardize")
# Feature importance of variables
imp_feature=generateFilterValuesData(trainTask, method=c("information.gain","chi.squared"))
plotFilterValues(imp_feature, n.show=20)
# Logistic regression
logistic=makeLearner("classif.logreg", predict.type="response")
validateTask=makeClassifTask(data=validate,target="class", positive="1")
# Loading GBM
getParamSet("classif.gbm")
library(e1071)
library(smotefamily)
library(ggplot2)
library(rgl)
library(misc3d)
library(leaps)
library(MASS)
library(mlr)
library(FSelector)
library(rpart)
library(gbm)
library(xgboost)
#parallÃ©liser
library(parallelMap)
#dÃ©marrer la parallÃ©lisation
parallelStartSocket(cpus=4)
library(caTools)
library(ROCR)
set.seed(12345)
######################################################################################################
################################## DEBUT #############################################################
######################################################################################################
zozo=read.table('C:/Users/Caroline/Documents/M2/SVM/projet/creditcard.csv',header=T,sep=",")
zozo=zozo[,-c(16,14,26,23,27)]
zozo$Class=as.factor(zozo$Class)
data=readRDS("C:/Users/Caroline/Documents/M2/SVM/projet/new.rds")
data$class=as.factor(data$class)
set.seed(12345)
taille_ech=175000
trainindex=sample(index,round(taille_ech*0.55))
train=data[trainindex,]
validateindex=sample(index,round(taille_ech*0.27))
validate=data[validateindex,]
itest=sample(index,round(taille_ech*0.18))
test=data[itest,]
index=1:nrow(data)
attach(train)
# Create a task
trainTask=makeClassifTask(data=train, target="class")
testTask=makeClassifTask(data=test, target="class")
validateTask=makeClassifTask(data=validate, target="class")
zozoTask=makeClassifTask(data=zozo, target="Class")
# Let's consider the positive class as 1
trainTask=makeClassifTask(data=train,target="class", positive="1")
testTask=makeClassifTask(data=test,target="class", positive="1")
validateTask=makeClassifTask(data=validate,target="class", positive="1")
zozoTask=makeClassifTask(data=zozo,target="Class", positive="1")
# Let's normalize the variables
trainTask=normalizeFeatures(trainTask,method="standardize")
testTask=normalizeFeatures(testTask,method="standardize")
validateTask=normalizeFeatures(validateTask,method="standardize")
zozoTask=normalizeFeatures(zozoTask,method="standardize")
# Feature importance of variables
imp_feature=generateFilterValuesData(trainTask, method=c("information.gain","chi.squared"))
plotFilterValues(imp_feature, n.show=20)
# Loading GBM
getParamSet("classif.gbm")
g.gbm=makeLearner("classif.gbm", predict.type="response")
# Specify the tuning method
rancontrol=makeTuneControlRandom(maxit=5)
# 3 fold CV
set_cv=makeResampleDesc("CV",iters=3)
# Set tunable parameters
gbm_par=makeParamSet(
makeDiscreteParam("distribution", values="bernoulli"),
makeIntegerParam("n.trees", lower=100, upper=500),
makeIntegerParam("interaction.depth", lower = 2, upper=10),
makeIntegerParam("n.minobsinnode", lower=10, upper=80),
makeNumericParam("shrinkage",lower=0.01, upper=1))
# Setting parameters
final_gbm=setHyperPars(learner=g.gbm,
par.vals=list(distribution="bernoulli", n.trees=256,
interaction.depth=5, n.minobsinnode=33,
shrinkage=0.244))
# Train
to.gbm=train(final_gbm, trainTask)
# Predicting on the test set
pr.gbm=predict(to.gbm, testTask)
# Submission file
submit6=data.frame(class = test$class, class_Status = pr.gbm$data$response)
table(submit6$class,submit6$class_Status)
mean(submit6$class==submit6$class_Status)
# Predicting on a new dataset
pr.gbm=predict(to.gbm, zozoTask)
# Submission file
submit6=data.frame(class=zozo$Class, class_Status=pr.gbm$data$response)
table(submit6$class,submit6$class_Status)
mean(submit6$class==submit6$class_Status)
# Loading XGBoost
set.seed(12345)
getParamSet("classif.xgboost")
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "response")
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "error",
nrounds = 250)
# Defining parameters for tuning
xg_ps=makeParamSet(
makeIntegerParam("nrounds",lower=200,upper=500),
makeIntegerParam("max_depth",lower=3,upper=20),
makeNumericParam("lambda",lower=0.55,upper=0.60),
makeNumericParam("eta", lower = 0.001, upper = 0.5),
makeNumericParam("subsample", lower = 0.10, upper = 0.80),
makeNumericParam("min_child_weight",lower=1,upper=5),
makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8))
# Defining search function
rancontrol=makeTuneControlRandom(maxit=5)
# 3 fold cross validation
set_cv=makeResampleDesc("CV",iters=3)
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# Train model
xgmodel=train(xg_new, trainTask)
# Predicting on the test set
predict.xg=predict(xgmodel, task=testTask)
# Submission file
submit7=data.frame(class = test$class, class_Status = predict.xg$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, task=zozoTask)
# Submission file
submit7=data.frame(class = zozo$Class, class_Status = predict.xg$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
#stopper la parallÃ©lisation
parallelStop()
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "response")
getParamSet("classif.xgboost")
# Loading XGBoost
set.seed(12345)
getParamSet("classif.xgboost")
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "response")
getParamSet("classif.xgboost")
g.gbm=makeLearner("classif.gbm", predict.type="response")
getParamSet("classif.xgboost")
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "response",monotone_constraint="numeric")
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "response",monotone_constraint="integer")
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "response",monotone_constraint="integervector")
# Make learner with inital parameters
xg_set=makeLearner("classif.xgboost", predict.type = "response",)
xg_set$par.vals=list(objective = "binary:logistic",
eval_metric = "error",
nrounds = 250)
# Defining parameters for tuning
xg_ps=makeParamSet(
makeIntegerParam("nrounds",lower=200,upper=500),
makeIntegerParam("max_depth",lower=3,upper=20),
makeNumericParam("lambda",lower=0.55,upper=0.60),
makeNumericParam("eta", lower = 0.001, upper = 0.5),
makeNumericParam("subsample", lower = 0.10, upper = 0.80),
makeNumericParam("min_child_weight",lower=1,upper=5),
makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8))
# Defining search function
rancontrol=makeTuneControlRandom(maxit=5)
# 3 fold cross validation
set_cv=makeResampleDesc("CV",iters=3)
# Setting parameters
xg_new=setHyperPars(learner=xg_set, par.vals=list(nrounds=256,max_depth=20,lambda=0.56,eta=0.278,subsample=0.56,min_child_weight=3.84,colsample_bytree=0.683))
# Train model
xgmodel=train(xg_new, trainTask)
# Predicting on the test set
predict.xg=predict(xgmodel, task=testTask)
# Submission file
submit7=data.frame(class = test$class, class_Status = predict.xg$data$response)
table(submit7$class,submit7$class_Status)
mean(submit7$class==submit7$class_Status)
# # Predicting on a new dataset
predict.xg=predict(xgmodel, task=zozoTask)
# Submission file
submit7=data.frame(class = zozo$Class, class_Status = predict.xg$data$response)
table(submit7$class,submit7$class_Status)
shiny::runApp('M2/SVM/projet/projetSVM/Onglet2')
install.packages('httr')
runApp('M2/SVM/projet/projetSVM/Onglet2')
runApp('M2/SVM/projet/projetSVM/onglet22')
runApp('M2/SVM/projet/projetSVM/onglet22')
runApp('M2/SVM/projet/projetSVM/sauvegarde onglet 2')
install.packages('ineq')
runApp('M2/SVM/projet/projetSVM/sauvegarde onglet 2')
runApp('M2/SVM/projet/projetSVM/onglet11')
runApp('M2/SVM/projet/projetSVM/Onglet1')
runApp('M2/SVM/projet/projetSVM/Onglet1')
runApp('M2/SVM/projet/projetSVM/Onglet1')
runApp('M2/SVM/projet/projetSVM/Onglet1')
runApp('M2/SVM/projet/projetSVM/Onglet1')
runApp('M2/SVM/projet/projetSVM/SVM-R-T-Y')
